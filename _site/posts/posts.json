[
  {
    "path": "posts/2021-05-24-wingspan-strategy-guide/",
    "title": "Wingspan Strategy Guide",
    "description": {},
    "author": [
      {
        "name": "Paulius Alaburda",
        "url": {}
      }
    ],
    "date": "2021-05-24",
    "categories": [],
    "contents": "\r\nWingspan is a bird game. It’s great!\r\nTypes of birds\r\nThe god tier\r\nThe food providers\r\nCaching Food\r\nThis includes the woodpeckers. The cards provide points situationally, making them inferior to egg laying birds. However, it’s not uncommon to run out of space for eggs, so these birds provide an alternative method.\r\nEgg-laying\r\nThis includes doves and quail. Prioritise these birds! They might seem weak because of the points values but they are definitely worth it over the whole course of the game.\r\nCard drawing\r\nThese birds can be split into a few categories. The cheap “all players draw” birds are great but the expensive ones aren’t. The “you draw a card” birds are only great if they are cheap. The early game is difficult because of food scarcity and spending more than 2 resources on a bird will set you back.\r\nFlocking\r\nI love these birds. They are usually cheap and they always provide a point when triggered. Furthermore, they give you the ability to cycle your hand. Cycling is important in Wingspan because most birds are situational and you want to go through as much of the deck as possible to get the right bird at the right time. That means you want to draw cheap birds at the start of the game and expensive birds at the end of the game.\r\nHunters/Fishers\r\nBonus drawers\r\nPlay a second bird\r\nJust like the “vanilla” birds, I found these to be great for late game plays when you have a surplus of food.\r\nHot tips\r\nYour focus should be the grasslands\r\nAt first, it may seem like the game offers four potential strategies: focusing on one of three biomes (the forest, grassland or wetland), or trying to diversify across all of them. However, the board is misleading. Unlike cards or food tokens, eggs provide points at the end of the game. Because of this, the “lay egg” action is the only action that can consistently provide points. The optimal strategy is to build your tableau so that you could repeatedly lay eggs during the last around and activate as many brown powers as possible.\r\nWhat’s worse, there are almost no forest birds that could turn food tokens into points efficiently. In the wetland, there are birds that allow you to tuck card to lay an egg - those cards are excellent! In fact, it’s usually easier to have the wetlands as a supporting biome that provides birds as well as a few points. Alternatively, if you do not have birds that provide food tokens, the forest biome should be your secondary biome instead. Essentially, the grasslands is your focus and one of the other biomes should support the grasslands.\r\nFinally, a potential strategy is to focus on high value birds and building as many of them as possible. At the end of the game, it’s frequently worth building one or two high scoring birds. However, a “build ALL the birds” strategy is inferior. To build your 4th bird, you will need to use 2 eggs. That effectively reduces the value of your played bird by 2 points. I believe that is only viable if you are playing either birds without effects (which give 7-9 points) or birds that draw additional bonus cards. Otherwise, laying is just better because usually you can lay 3-4 eggs and trigger a brown effect for an additional point. In the later stages of the game, playing anything worth less than 5 points is a lot of the time going to be worse than laying eggs.\r\nWhere to place brown, white, pink etc. powers?\r\nWith our “grassland first” strategy, we would be planning on triggering egg laying as much as possible. Because of this, it is best to put as many birds with brown powers as possible into the grasslands. Birds with white, teal or pink powers should be placed in the secondary or even tertiary biome. There is no use to put birds without brown powers in biomes you are planning to trigger frequently.\r\nAction diversification is not always beneficial\r\nAfter my first few games of Wingspan, I thought that putting different brown powers into one biome is key. If you can lay eggs, get food and draw cards in one action, that’s great, isn’t it? Because of this, some of the birds described previously are game-breaking. However, it’s better to put birds that give points into the grassland, birds with card draw into the wetland and birds with food token abilities into the forest. That way you can spend one action to get all the food or cards you need. Furthermore, this makes sure that laying eggs gives the most amount of points - remember, food tokens and cards are useless at the end of the game!\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-07T22:20:55+03:00",
    "input_file": "index.utf8.md"
  },
  {
    "path": "posts/2021-03-09-3-ways-to-turbocharge-your-excel-team/",
    "title": "3 Ways to Turbocharge Your Excel Team",
    "description": {},
    "author": [
      {
        "name": "Paulius Alaburda",
        "url": {}
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\r\nA lot of companies still have Excel teams working with data. It may seem like folks working in the Microsoft garden are missing out on the bees’ knees of R, Python or Julia, but Excel itself is also under active development. To be honest, the way I used Excel 2 years ago is completely different from how I use it know. What’s more, Excel has become part of a larger ecosystem of tools. I think every Excel team could adopt them to supercharge their work!\r\n1. PowerQuery\r\nI like to imagine Excel standing on three columns: the usual formulas, VBA and PowerQuery. While formulas are quick and dirty, nested formulas can become a drag to maintain quite quickly. VBA does everything until the one or two VBA gurus leave your team. I feel like PowerQuery strikes a great balance between the two: it provides more functionality than formulas, the steps to transform your data are recorded in one place and maintenance is much easier. PowerQuery is, essentially, a way to write a reproducible data transformation pipeline.\r\nPowerQuery is based on the M language. It supports function definitions that you can call and comments are rendered alongside the pipeline. It doesn’t have everything you would expect from a programming language but having everything in one place forces the user to separate the input data from the way it is handled. This is incredibly useful whenever the data is replaced because your scripted logic stays intact.\r\nFinally, Power BI runs the same M language under the hood whenever it imports data. One immediate benefit is that you can copy PowerQuery code from Excel to Power BI and it just works. But that also means that you can establish a common language between your Excel teams and your Business Intelligence teams.\r\n2. Adopt Power Automate to reduce manual work\r\nMicrosoft seems to be on a course to bring PowerQuery, Power BI and Power Automate into a unified ecosystem. There are quite a few differences currently but Power Automate still has a ton of advantages. If you are an Excel team, chances are you may be dealing with manual work. It may be downloading Excel attachments, copy/pasting data or downloading responses from Microsoft Forms. Power Automate is the glue that can bring data over to your other tools.\r\nPower Automate is low-code/no-code application that can run pipelines on a schedule or based on a trigger. For example, we receive daily emails with an attached Excel file. Power Automate can download the attachment into a folder. Then PowerQuery can pull all the files from the folder into a single dataset for later use.\r\nPower Automate expands its integrations quite fast. Chances are you will find possible integrations across your stack. I was surprised to find you can pull responses from a submission form, munge them into a JSON object and push them through the Jira API as a user story. You may need a server instance for some of the tasks but for others, Power Automate is definitely a great tool.\r\n3. Adopt Tables!\r\nNot like from Ikea, but adopt working with Excel tables. If Power Query and Power Automate are too “out there”, I would encourage everyone to start formattng their datasets as tables. There are two key advantages to this.\r\nFirst, a table can be thought of as a sheet in a sheet. Whenever you try to query data from an Excel file, you can either query the whole sheet, or query a table. Querying a sheet can sometimes backfire, e.g. we had a formatting issue which forced PowerQUery to import over a million rows and 300 columns. A table has a clear definition and you can be sure you will import the same structure every time.\r\nSecond, instead of refering to cell ranges in Excel, you can refer to column names in the table. Suddenly, =SUM(A1:A5) becomes =SUM([sales]). This immediately benefits formula readability and maintenance because instead of checking the referenced cell range, you can immediately grasp the intention of the formula.\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-07T21:37:01+03:00",
    "input_file": "index.utf8.md"
  },
  {
    "path": "posts/2020-10-25-visualising-ess-data-using-ggplot2-and-tmap/",
    "title": "Visualising ESS data using ggplot2 and tmap",
    "description": {},
    "author": [
      {
        "name": "Paulius Alaburda",
        "url": {}
      }
    ],
    "date": "2020-10-25",
    "categories": [
      "r"
    ],
    "contents": "\r\n\r\n\r\n\r\nI stumbled across rOpenSci’s package essurvey that contains data from the European Social Survey (ESS) for European countries, including Lithuania! Also, tmap has a lot of excellent resources to get a head start, so this post shows how to access ESS data and visualise it with tmap.\r\nDownloading ESS data\r\nDownloading ESS data is fairly straightforward - register your email in the ESS and start pulling data! I recommend installing the development version because the stable version was downloading .zip files that Windows couldn’t extract. To replicate this post, you should use:\r\n\r\n\r\nimport_country(\"Lithuania\", 9)\r\n\r\n\r\n\r\nI didn’t want to disclose my email here, so I have downloaded the data separately.\r\n\r\n\r\n\r\nSo what is the ESS anyway?\r\nVisualising with tmap\r\nAll plotting packages depend on shapefiles. For Europe, it’s best to download the map from GISCO. However, the map does not have municipality data for Lithuania. Fortunately, the shapefile is available over the OpenMap project.\r\nThis is how to build a basic tmap plot:\r\n\r\n\r\nlt_map <- read_sf(\"data/admin_ribos.shp\")\r\n\r\nlt_map_sen <- lt_map %>% filter(ADMIN_LEVE == 4)\r\n\r\nnews <- ess %>% \r\n  group_by(region = labelled::to_factor(region)) %>% \r\n  summarise(`Internet usage` = mean(netustm, na.rm = TRUE)) %>% \r\n  # count(region = labelled::to_factor(region), netusoft) %>%\r\n  mutate(region = as.character(region)) %>% \r\n  mutate(region = case_when(region == \"Klaipedos apskritis\" ~ \"Klaipėdos apskritis\",\r\n                            region == \"Šiauliu apskritis\" ~ \"Šiaulių apskritis\",\r\n                            region == \"Taurages apskritis\" ~ \"Tauragės apskritis\",\r\n                            region == \"Telšiu apskritis\" ~ \"Telšių apskritis\",\r\n                            TRUE ~ region))\r\n\r\nlt_plot <- lt_map_sen %>% left_join(news, by = c(\"NAME\" = \"region\"))\r\n\r\ntm_shape(lt_plot) +\r\n  tm_fill(\"Internet usage\") +\r\n  tm_polygons()\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-10-25-visualising-ess-data-using-ggplot2-and-tmap/index_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-06-07T22:14:39+03:00",
    "input_file": "index.utf8.md"
  },
  {
    "path": "posts/2020-05-26-log-x-1-and-log-1-x/",
    "title": "log(x+1)* and log(1+x)*",
    "description": {},
    "author": [
      {
        "name": "Paulius Alaburda",
        "url": {}
      }
    ],
    "date": "2020-05-26",
    "categories": [
      "r",
      "statistics"
    ],
    "contents": "\r\nI’ll be honest, my math skills are not great and I haven’t studied maths formally since high school. However, Lior Pachter’s preprint brings a methodological issue in science through a mathematical lens really well. I had to look up a few things and I think it might be worth exploring the paper through the lens of a simulation.\r\nSo the first equation in the paper looks like this:\r\n\\[E[X|X > 0] = \\frac{\\lambda}{1-e^{-\\lambda}} \\] Essentially, what is expected value of a Poisson random variable after you filter out any non-zero results? If \\(\\lambda\\) is infinitely large, the expected value will approach \\(\\lambda\\), however, what happens if it approached zero? Using L’Hôpital’s rule we can estimate that the expected value will be roughly equal to 1. Here’s a quick simulation to confirm this:\r\n\r\n\r\nlibrary(purrr)\r\nlibrary(tidyverse)\r\n\r\n\r\ndf <- data.frame(Lambdas = c(0.00001,0.0001,0.0005,0.001,0.002,seq(from = 0.0001, to = 10, length.out = 30))) %>% \r\n  mutate(sim_data = map(Lambdas, ~rpois(100,.))) %>%\r\n  unnest(sim_data) %>% \r\n  group_by(Lambdas) %>% \r\n  mutate(`Expected value` = mean(sim_data)) %>%\r\n  ungroup() %>% \r\n  group_by(Lambdas,sim_data > 0) %>% \r\n  mutate(`Expected value when X > 0` = mean(sim_data)) %>%\r\n  ungroup() %>% \r\n  filter(`sim_data > 0` == TRUE) %>% \r\n  select(Lambdas,`Expected value`,`Expected value when X > 0`) %>% \r\n  unique() %>% \r\n  gather(key = \"Measurement\", value = \"Mean\", `Expected value`,`Expected value when X > 0`)\r\n  \r\ndf %>% ggplot(aes(x = log(Lambdas), y = Mean, color = Measurement)) + geom_line(alpha = 0.5, size = 1) + theme_bw() + labs(title = \"When lambda is sufficiently large, filtering zero counts has no effect\") + scale_y_continuous(breaks = c(1:10))\r\n\r\n\r\n\r\n\r\nThe paper points out a great example - in cases of gene expression, \\(\\lambda\\) can be small and filtering out zero counts (which could be wrongly interpreted as absence of the gene) will make lower gene expression indistinguishable from higher gene expression when both of them are low.\r\nThere’s a second issue when you try to log transform your counts. Log(0) is undefined, so what usually happens is that all the counts are incremented by one. This essentially means that we are filtering out zero counts and replacing them with ones. As we have seen for large lambdas, that does not really have an effect but it does have a large effect when we are dealing with rare events.\r\nThis is important in fields outside of RNA sequencing as well. For example, morphological studies can deal with cell count data as well as counts of certain structures of interest. I vaguely remember having to deal with count data during my undergraduate research years and failing to apply proper methods. If you are an undergrad reading this - do not log transform your count data if you think the event of interest is rare!\r\nDo read the whole thing, it’s short and brings references to explore further. Quite recommended.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-07T21:54:29+03:00",
    "input_file": "index.utf8.md"
  },
  {
    "path": "posts/2020-05-16-assumptions-of-the-chi-squared-test/",
    "title": "Assumptions of the Chi-squared test",
    "description": {},
    "author": [
      {
        "name": "Paulius Alaburda",
        "url": {}
      }
    ],
    "date": "2020-05-16",
    "categories": [
      "r",
      "statistics"
    ],
    "contents": "\r\nHaving written the post about the Chi-squared test, I thought I would list the assumptions of the Chi-square test and null hypothesis testing in general.\r\nSo the one thing I was taught was that the p value is the probability of getting the observed data if you were to repeat the experiment lots more. I don’t think I fully understood the implications of that. The \\(\\chi^2\\) value you calculate is basically just one value out of the whole distribution and doing the test does not explain the origin of the data. In a way, a survey can be thought of a data generation process. If you get a high Chi-squared value and a significant p value, you assume that the data in the survey are associated. However, 5 per cent of the time independent data will yield large Chi-squared values. Replication here would be really helpful but usually it doesn’t happen.\r\nFurthermore, the Chi-squared distribution is continuous (as in, it can have any value) while both Poisson and the Binomial are discrete (they only generate whole numbers) distributions. If your sample is small, the distributions do not converge to the normal distribution and the calculated Chi-squared value then does not really follow a Chi-squared distribution. Violating the assumption of sufficient sample size actually inflates the test statistic (the goodness of fit value and the Chi-squared test value). That in turn returns significant p values more often, giving false positive results. The answer to this has been Yates’ continuity correction in 1934 but the correction is basically a hack1.\r\nFinally, the Chi-squared test assumes the data are independent. If you sample the same hospital or your observations are related, you will tend to drive up your false positive rate as well.\r\nThere are a few solutions, fortunately. One, you can use the Fisher’s exact test for small sample sizes. Also, if you think your data is associated (or rather, paired), you could use the McNemar test or the Cochran test. The latter is often used to compare agreement between data and can frequently be found in methods papers to show that the method works similarly regardless of the person using it.\r\n\r\nLearning with R by Danielle Navarro explain this in more detail.↩︎\r\n",
    "preview": {},
    "last_modified": "2021-06-07T21:53:26+03:00",
    "input_file": "index.utf8.md"
  },
  {
    "path": "posts/2020-05-16-how-to-chi-squared-test/",
    "title": "How to Chi-squared test",
    "description": {},
    "author": [
      {
        "name": "Paulius Alaburda",
        "url": {}
      }
    ],
    "date": "2020-05-16",
    "categories": [],
    "contents": "\r\nAs I am working through Statistical Rethinking and picking up new tools, I decided to rework my knowledge of null hypothesis testing. This is definitely not the worst tutorial on the Chi-squared test but this is more of a write up to wrap my head around it. I think this will of interest1 for first year undergraduates or students interested in competing in biology olympiads.\r\nPremise of the Chi-squared test\r\nKearl Pearson created the test in 1900 to answer the question whether two or more categorical variables are associated with one another. It is built on the idea that the sum of squares of independent normal distributions is distributed as a Chi-squared distribution. If the distributions were dependent, then their sum of squares would not follow a Chi-squared distribution.\r\nNow, we usually cannot know the whole distribution and instead we collect samples via surveys and experiments (as in, we usually don’t know the exact gender, political affiliation or age breakdown of the whole population we are studying). However, we do know what a Chi-squared distribution should like under the null hypothesis of there being no association. So by using the test we get a single chi-squared value and check where that value falls in the Chi-squared distribution. Through that, we can calculate the probability of getting a value equal to or greater than the value we calculated, i.e. the p value.\r\nStep by step of the Chi-squared test\r\nThat sounds a little too much for me and probably for a lot of students out there as well. What usually happens is that a workflow is taught instead: gather categorical data, shove that into the Chi-squared test formula and stick your Chi-squared value and your p value into the paper2.\r\nIn my case, I was taught how to perform the Chi-squared test by hand. That’s actually great because it lets us breakdown the Chi-squared test formula3:\r\n\\[\\tilde{\\chi}^2=\\sum_{k=1}^{n} \\frac{(O_k - E_k)^2}{E_k}\\]\r\nHere \\(O_k\\) is the observed count of a category, \\(E_k\\) is the expected count.\r\nLet’s say you are given a contingency table of two variables (in this case, the number of cylinders and the number of gears from the mtcars dataset).\r\nFirst, create a contingency table. This shows our observed values \\(O_k\\) for each combination of gear and cylinder:\r\n\r\n\r\nlibrary(tidyverse)\r\n\r\nobserved <- table(mtcars$cyl, mtcars$gear)\r\n\r\nobserved %>% knitr::kable()\r\n\r\n\r\n\r\n3\r\n4\r\n5\r\n4\r\n1\r\n8\r\n2\r\n6\r\n2\r\n4\r\n1\r\n8\r\n12\r\n0\r\n2\r\n\r\nThen, find the expected values \\(E_k\\) as though there was no association between the two. If you were forced to do this with pen and paper, you would have to calculate the proportion of each type of gear and each type of cylinder. Using R, here is the proportion of each type of cylinder:\r\n\r\n\r\ncyl_summary <- mtcars %>% count(cyl) %>% mutate(Proportion = prop.table(n))\r\n\r\ncyl_summary %>% knitr::kable()\r\n\r\n\r\ncyl\r\nn\r\nProportion\r\n4\r\n11\r\n0.34375\r\n6\r\n7\r\n0.21875\r\n8\r\n14\r\n0.43750\r\n\r\nAnd each type of gear:\r\n\r\n\r\ngear_summary <- mtcars %>% count(gear) %>% mutate(Proportion = prop.table(n)) \r\n\r\ngear_summary %>% knitr::kable()\r\n\r\n\r\ngear\r\nn\r\nProportion\r\n3\r\n15\r\n0.46875\r\n4\r\n12\r\n0.37500\r\n5\r\n5\r\n0.15625\r\n\r\nIf there was no association between gear and cylinder, we would expect that the probability of a 4 cylinder car with 3 gears would be the probability of a 4 cylinder car times the probability of 3 gear car. You can have R this for you for each combination:\r\n\r\n\r\ntbl_prop <- cyl_summary$Proportion %o% gear_summary$Proportion\r\nrownames(tbl_prop) <- c(4,6,8)\r\ncolnames(tbl_prop) <- c(3,4,5)\r\n\r\ntbl_prop %>% knitr::kable()\r\n\r\n\r\n\r\n3\r\n4\r\n5\r\n4\r\n0.1611328\r\n0.1289062\r\n0.0537109\r\n6\r\n0.1025391\r\n0.0820312\r\n0.0341797\r\n8\r\n0.2050781\r\n0.1640625\r\n0.0683594\r\n\r\nTo get the expected values \\(E_k\\), we can multiply the probabilities by the number of total observations (in this case, 32):\r\n\r\n\r\nexpected <- tbl_prop*sum(observed)\r\n\r\nexpected %>% knitr::kable()\r\n\r\n\r\n\r\n3\r\n4\r\n5\r\n4\r\n5.15625\r\n4.125\r\n1.71875\r\n6\r\n3.28125\r\n2.625\r\n1.09375\r\n8\r\n6.56250\r\n5.250\r\n2.18750\r\n\r\nThen calculate the difference between the observed and the expected values. This gives you a table of differences between \\(O_k\\) and \\(E_k\\):\r\n\r\n\r\n(observed-expected) %>% knitr::kable()\r\n\r\n\r\n\r\n3\r\n4\r\n5\r\n4\r\n-4.15625\r\n3.875\r\n0.28125\r\n6\r\n-1.28125\r\n1.375\r\n-0.09375\r\n8\r\n5.43750\r\n-5.250\r\n-0.18750\r\n\r\nFor example, we did not observe any cars with 8 cylinders and 4 gears but based on the how frequent 8 cylinder cars and 4 gear cars are, we expected to see 5.25 cars.\r\nNow what’s interesting is that the sum of all those differences will always sum to 0. That is inconvenient, so what Pearson did in 1900 is square the differences, giving us this:\r\n\r\n\r\n(observed-expected)^2 %>% knitr::kable()\r\n\r\n\r\n\r\n3\r\n4\r\n5\r\n4\r\n17.274414\r\n15.015625\r\n0.0791016\r\n6\r\n1.641602\r\n1.890625\r\n0.0087891\r\n8\r\n29.566406\r\n27.562500\r\n0.0351562\r\n\r\nDivide by the expected value - without this step \\((O_k-E_k)^2\\) is just proportional to the number of counts, so comparing, for example, a sample of a 1000 counts and 100 counts would be impossible. That is why you need to normalise the values by dividing by the expected value4.\r\n\r\n\r\n((observed-expected)^2/expected) %>% knitr::kable()\r\n\r\n\r\n\r\n3\r\n4\r\n5\r\n4\r\n3.3501894\r\n3.6401515\r\n0.0460227\r\n6\r\n0.5002976\r\n0.7202381\r\n0.0080357\r\n8\r\n4.5053571\r\n5.2500000\r\n0.0160714\r\n\r\nSum the resulting values:\r\n\r\n\r\nchisq_rs <- sum((observed-expected)^2/expected)\r\nprint(chisq_rs)\r\n\r\n\r\n[1] 18.03636\r\n\r\nUsing a reference table (for example, this one one), find the critical p value (in this case, for df = 2). You do that by finding the largest Chi-squared value that is smaller than the one we calculated (in this case, smaller than 18.04). In this case, 18.04 for df = 2 is larger than the critical value of 9.21 so we can say that the p value is less than 0.01.\r\nTo be exact, let’s see where this value falls if we were to build a Chi-squared distribution with df = 2:\r\n\r\n\r\nchisq_sample <- rchisq(10000, 2)\r\n\r\nqplot(chisq_sample) + geom_vline(xintercept = chisq_rs, color = \"red\") + theme_bw()\r\n\r\n\r\n\r\nchisq_ecdf <- ecdf(chisq_sample)\r\nchisq_percentile <- chisq_ecdf(chisq_rs)\r\n\r\n\r\n\r\nOr, if we built a distribution function out of our samples, our value would fall into the 0.9997 percentile. Since that is definitely larger than 0.95, we can conclude that there is an association between the number of gears and the number of cylinders. Alternatively, this is what the Chi-squared test would have returned:\r\n\r\n\r\nrs <- chisq.test(observed)\r\nrs\r\n\r\n\r\n\r\n    Pearson's Chi-squared test\r\n\r\ndata:  observed\r\nX-squared = 18.036, df = 4, p-value = 0.001214\r\n\r\nWhich we could have reported as such (while also describing the count data):\r\n\r\nA chi-square test was conducted whether there is an association between the number of gears and the number of cylinders across different cars. The results were significant (χ2(4) = 18.04, p = 0.001)\r\n\r\nBut why does it work?\r\nI’ll be honest, it takes me a while to grok distributions beyond the Binomial and the Gaussian. A good explanation how the Chi-squared distribution is derived can be found here.\r\nThe chi-square (\\(\\chi^2\\)) distribution, as we have mentioned earlier, is a distribution of the sum of squares of two or more normal distributions. In other words, if you would square values from one normal distribution and add it to another, the result should approximate the Chi-square distribution:\r\n\\[U∼N(0,1)\\] \\[V∼N(0,1)\\] \\[U^2+V^2∼\\chi_2^2\\]\r\nThat subscript denotes that the sum of squares follows a \\(\\chi^2\\) distribution with 2 degrees of freedom. Alternatively, a square of just one normal distribution follows a \\(\\chi_1^2\\) distribution and for any number of degrees:\r\n\\[\\sum_{j=1}^{k} \\chi_j^2∼\\chi_k^2\\]\r\nLet’s plot these distributions!\r\n\r\n\r\ndf <- tibble(`Normal` = rnorm(10000, mean = 0, sd = 1),\r\n             `Normal squared` = Normal^2,\r\n             `Actual Chi-squared` = rchisq(10000,2),\r\n             `Chi-squared with df = 2` = `Normal squared` + rnorm(10000, mean = 0, sd = 1)^2) %>%\r\n  gather(key = \"distribution\", value = \"value\") %>% \r\n  mutate(distribution = factor(distribution, levels = c(\"Normal\",\"Normal squared\",\"Chi-squared with df = 2\",\"Actual Chi-squared\")))\r\n\r\nggplot(df, aes(x = value, fill = distribution)) + \r\n  geom_histogram() + \r\n  facet_grid(~distribution, scales = \"free_x\") + \r\n  guides(fill = FALSE) + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nSo… Now what? How does any of this relate back to the Chi-squared test formula? Here’s where the central limit theorem comes in: even though any count data (whether it’s counting blood cells, people, adverse events) is distributed along a poisson distribution, the sampling distribution (as in, the result of collecting samples a lot of times) is similar to the normal distribution.\r\nTo understand this, I’ve simulated two Poisson distributions (as in, count data). If we tried to build a Chi-squared distribution out of this data, what percentage of values would be higher than the 95th percentile of an actual Chi-squared distribution?\r\n\r\n\r\ndf <- tibble(\r\n  pois_1 = rpois(10000, 10),\r\n  pois_2 = rpois(10000, 10),\r\n  chisq_real = rchisq(10000, df = 2)\r\n  ) %>% \r\n  mutate(pois_1 = (pois_1-mean(pois_1))/sd(pois_1),\r\n         pois_2 = (pois_2-mean(pois_2))/sd(pois_2)) %>% \r\n  mutate(chisq_11 = pois_1^2+pois_1^2,\r\n         chisq_12 = pois_1^2+pois_2^2) %>% \r\n  select(contains(\"chisq_\")) %>% \r\n  gather(key = \"distribution\", value = \"value\") %>%\r\n  mutate(over_95 = value > qchisq(.95, df=2))\r\n\r\nggplot(df, aes(x = value, fill = over_95)) + \r\n  geom_histogram(alpha = 0.5, binwidth = 1) + \r\n  facet_grid(~distribution) + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nchisq_12 is built via two independent Poisson distributions, while chisq_11 is just to identical samples added together (making the distributions dependent). From the chart it seems like it works and if you look at the percentage of values higher than the 95th percentile:\r\n\r\n\r\ndf %>% \r\n  group_by(distribution) %>% \r\n  summarise(over_95 = mean(over_95)) %>% \r\n  knitr::kable()\r\n\r\n\r\ndistribution\r\nover_95\r\nchisq_11\r\n0.0771\r\nchisq_12\r\n0.0488\r\nchisq_real\r\n0.0518\r\n\r\nSo chisq_12 and chisq_real makes sense - around 5 per cent of values are going to be higher than the 95th percentile. For chisq_11, we see that is not the case and the percentage is higher!\r\nWe can see that the Chi-squared distribution requires that the added distributions be independent. The final kicker comes in when we look back at the Chi-squared test formula:\r\n\\[\\tilde{\\chi}^2=\\sum_{k=1}^{n} \\frac{(O_k - E_k)^2}{E_k}\\] If the difference between observed and expected values is small, then \\(\\chi^2\\) is small. If the difference is large, then \\(\\chi^2\\) is large. The observed values follow a normal distribution and the expected values also follow a normal distribution, therefore the squared difference can be modeled as a Chi-squared distribution. And since it can be modeled it can be turned into a probability of observing this or more extreme5 data!\r\nTo me this makes enough sense to trust it. In summary, Pearson created the formula that returns a value. This particular value increases or decreases based on the difference of observed and expected count data. The test statistic by itself does not tell us much, but we know that the formula should generate values that are distributed along a chi-squared distribution and we can translate the value into a probability by checking where Chi-squared test values fall in the Chi-squared test distribution.\r\nEven though the test is pretty simple, it packs a lot of assumptions and nuance. For a really good explanation, I would recommend checking out Danielle Navarro’s book Learning with R.\r\n\r\nNot necessarily interesting, haha↩︎\r\nThis would make for a great “HAHA CHI-SQUARED TEST GOES BRRR” meme.↩︎\r\nI am leaving out some nuance here by using the goodness of fit formula when I could also show the Test for Independence formula which looks like this: \\(X^2=\\sum\\limits_{i=1}^I \\sum\\limits_{j=1}^J\\dfrac{(O_{ij}-E_{ij})^2}{E_{ij}}\\) Although it is more correct, it also takes more time to read and understand for people not used to indices.↩︎\r\nA really good technical explanation can be found here. Essentially \\(O_k\\) is a count therefore it can be modeled as a Poisson distribution (\\(O_k ∼ Poisson(E_k,E_k)\\)) and with high enough counts the distribution becomes similar enough to the normal distribution with mean and variance \\(E_k\\).↩︎\r\nI am putting this clunky wording here just to be correct because the probability of observing an exact Chi-squared test value is always zero since the distribution is continuous.↩︎\r\n",
    "preview": "posts/2020-05-16-how-to-chi-squared-test/index_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-06-07T21:53:55+03:00",
    "input_file": "index.utf8.md"
  },
  {
    "path": "posts/2020-01-15-student-reported-p-values/",
    "title": "Student reported p values",
    "description": {},
    "author": [
      {
        "name": "Paulius Alaburda",
        "url": {}
      }
    ],
    "date": "2020-01-15",
    "categories": [
      "r",
      "statistics",
      "education"
    ],
    "contents": "\r\nI have been delaying this post for 2 years but I am glad to finally publish it. This post is an example of incentives driving the students to publish more abstracts with more p values in them and I think the charts tell pretty much the whole story.\r\n\r\n\r\nknitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)\r\n\r\nlibrary(tidyverse)\r\nlibrary(lme4)\r\nlibrary(stats4)\r\nlibrary(knitr)\r\n\r\nabstract_data <- read_csv(\"https://raw.githubusercontent.com/Alaburda/thesisExtractr/master/abstract_data.csv\")\r\n\r\npvalue_data <- read_csv(\"https://raw.githubusercontent.com/Alaburda/thesisExtractr/master/p_vals.csv\")\r\n\r\n\r\n# Additional wrangling\r\n## Create rounded variable\r\npvalue_data$rounded <- 0\r\n\r\n## Being rounded and being truncated is mutually exclusive, so take untruncated values and see if they are rounded\r\npvalue_data$rounded[pvalue_data$truncated == 0] <- ifelse(round(pvalue_data$pval[pvalue_data$truncated == 0],2) == pvalue_data$pval[pvalue_data$truncated == 0],1,0)\r\n\r\n# In cases such as \"p < 0.00\", the p value is reported incorrectly. The p value in that case is actually rounded and not truncated (even though it is reported in a truncated manner).\r\npvalue_data[pvalue_data$truncated == 1 & pvalue_data$pval == 0, ] <- \r\npvalue_data %>% filter(truncated == 1, pval == 0) %>% mutate(truncated = 0,\r\n                                                             rounded = 1)\r\n\r\n# Mean imputation for p values?\r\n\r\n# Live to check papers with confidence intervals but no p values\r\n#anti_join(abstract_data,pvalue_data,by = c(\"ID\",\"Year\")) %>% filter(grepl(\"95%\",Results)) %>% select(-Methods) %>%  View()\r\n\r\n\r\n\r\nIn 2014, Jeff Leek and Leah R. Jager published a paper about the rate of false positive findings in research papers. The idea was to scrape multiple journal paper abstracts and estimate the number of false positive results. The paper was really interesting to read and inspired me to try to do something similar with a corpus of abstracts from my university. Over here, we have an annual local conference for medical students to submit their research abstracts. Students are incentivised to submit because a printed abstract goes towards increasing your rating when entering residency. Luckily, the printed abstract books follow a similar pattern of introduction-methods-results-conclusion throughout the years, giving us an opportunity to show publishing trends over time.\r\nFirst things first, the code to parse abstracts and extract p values can be found in a separate [repo here] (https://github.com/Alaburda/thesisExtractr). The code may not be perfectly tidy due to differences in abstract books, but should be able to run and recreate the datasets. In principle, it is very similar to what Jeff Leek and Leah R. Jager had done but I had to make a few language-specific additions. First of all, the code parses the abstract book into chunks of text based on the words “Vadovas” (eng. supervisor), “Autoriai” (eng. authors), “Metodai” (eng. Methods) and “Rezultatai” (eng. Results). Separating methods from the results was crucial because most methods mention using p < 0.05 as their threshold for significance, giving us a correct estimate of the number of p values reported in each abstract.\r\nThe results section was parsed for three types of p values. One group are the truncated and significant p values (e.g. p < 0.05). P values reported as p0.05 were also included in this group on the assumption that the special symbol ≤ was somehow lost during the submission process. The second group of p values are truncated but insignificant p values (p > 0.05). Finally, the final group are exact untruncated p values (e.g. p = 0.04).\r\nThe final data has two datasets. One is the list of abstracts from each year with its corresponding year, ID, methods and results. The other datasets contains the parsed p values with the corresponding year, ID and information whether the p value is significant, truncated and rounded.\r\nHere’s what the data looks like:\r\n\r\nID\r\nYear\r\ntruncated\r\npval\r\nsignificant\r\nrounded\r\n6\r\n2004\r\n0\r\n0.0002\r\n1\r\n0\r\n6\r\n2004\r\n0\r\n0.0007\r\n1\r\n0\r\n6\r\n2004\r\n0\r\n0.5100\r\n0\r\n1\r\n6\r\n2004\r\n0\r\n0.1370\r\n0\r\n0\r\n6\r\n2004\r\n0\r\n0.4600\r\n0\r\n1\r\n6\r\n2004\r\n0\r\n0.0070\r\n1\r\n0\r\n\r\nAbstract results\r\n\r\n\r\n\r\nThe number of abstracts increased drastically in 2012 to a whooping 559 abstracts. The increase persisted until 2016 - this was when an international conference had started being organised. The way the system works is that you are awarded more points for submitting to international conferences. My guess is, students skipped the local conference to present at the international one instead.\r\nP value results\r\nThe plot below shows the percentage of significant p values reported each year:\r\n\r\n\r\n\r\nThe percentage of positive p values increased to around 50 percent and plateaued between 43 and 50 per cent. It’s also interesting to note that 2013 through 2015 had the most abstracts, perhaps diluting positive results.\r\nHow many abstracts reported p values in the first place though?\r\n\r\n\r\n\r\nNow that’s beautiful - if I recall correctly from my student years, at some point guidelines started requiring reporting statistical results (in other words, p values) rather than outlining general findings. This would explain the tendency for students to almost uniformly start reporting p values in their papers.\r\n\r\n\r\n\r\nNot only has the number of p values increased, the average abstract also started reporting more p values in an abstract. I am not sure what is going on in 2019 but then again - an average of over 6 p values in an abstract is pretty crazy. We could double check whether this is due to a larger sample of outliers:\r\n\r\n\r\n\r\nThe distributions are somewhat consistent across the years, suggesting that generally students may have started overreporting p values. Given that, you could suspect that people would report truncated values instead of exact ones.\r\n\r\n\r\n\r\nHowever, we see that instead of reporting truncated p values, students have instead started reporting exact values more often. Despite the overreliance on p values, it is great to see that students have adopted a better standard to report them!\r\nOverall, this shows that statistical education in medicine has been getting better but there are still some ways to go. Students rely on p values (and rely on reporting a lot of them) to legitimise their work. Change, however, requires both institutional and student involvement. With conference policy requiring statistical tests in abstracts, I don’t see this tendency going away any time soon and I will be sure to give an update once the 2020 conference hits.\r\nIs it possible to estimate a false discovery rate from p values?\r\nWell, it’s complicated and it needs some explaining to do. First of all, if we want to figure out the proportion of false positive results (FDR) from the array of reported p values, we need to know the underlying distribution of p values and we would ideally like to know the effect sizes of the studies. As from this blogpost, as the effect size decreases, the FDR tends to increase with the same p values. So without knowing the effect size reported in the abstracts, we cannot make fair judgments. This problem is further complicated by the fact that effect sizes tend to be larger when the results are significant. This makes sense because for a large effect size to be statistically significant, there is more tolerance for larger errors.\r\nSecondly, what is the underlying distribution of p values we are trying to analyse? We usually expect the distribution of p values to be uniform but students are biased to publish positive results rather than negative results. Furthermore, it is not uncommon to consciously or unconsciously p-hack your way to statistical significance. Remember, there is pressure to ensure the abstracts are accepted.\r\nBecause of these reasons, I decided to skip publication bias for now and focus on the exploratory side of things. Maybe in another blog post! Also, if you would like to read more, be sure to read some of the sources below.\r\nSources:\r\nhttps://academic.oup.com/biostatistics/article/15/1/1/244509\r\nhttps://github.com/jtleek/tidypvals\r\nhttp://varianceexplained.org/statistics/interpreting-pvalue-histogram/\r\nhttp://jtleek.com/genstats_site/lecture_notes/03_12_P-values.pdf\r\nhttps://www.nature.com/news/statistics-p-values-are-just-the-tip-of-the-iceberg-1.17412\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-07T21:47:03+03:00",
    "input_file": "index.utf8.md"
  },
  {
    "path": "posts/2018-09-10-analysing-lsmu-publications/",
    "title": "Analysing LSMU publications",
    "description": {},
    "author": [
      {
        "name": "Paulius Alaburda",
        "url": {}
      }
    ],
    "date": "2018-09-10",
    "categories": [
      "r",
      "lsmu"
    ],
    "contents": "\r\nHello! I have recently finished my master’s degree and finished my summer projects! With spare time on my hands, I have finally gotten around to analysing the full list of my university’s publications.\r\nI have run into this dataset during a seminar about InCites, a tool for researchers to look into their university’s research, collaborations with other universities and even funding sources. The tool itself is neat, but I saw this as an opportunity to use R and to get to know my university a little better!\r\nAbout my university, the Lithuanian University of Health Sciences\r\nThe Lithunian University of Health Sciences is one of the biggest universities in the Baltic region. As the name implies, the university specialises in biomedical sciences. In fact, the university attracts the most high school students to study medicine in the country. Historically, the university dates back to the 1920s when it was founded as a medical academy. Later on, in 1998, it was renamed the Kaunas University of Medicine. Finally, the university merged with the Lithuanian Veterinary Academy in 2010 and became the Lithuanian University of Health Sciences.\r\nThe dataset itself\r\nThe dataset contains every indexed research paper that contains a co-author from the Lithuanian University of Health Sciences. The total number of publications as of May 31st is 5578 and the database was last updated on July 28th.\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(tidytext)\r\nlibrary(widyr)\r\nlibrary(igraph)\r\nlibrary(ggraph)\r\nlibrary(highcharter)\r\nlibrary(RColorBrewer)\r\nlibrary(colorRamps)\r\nlibrary(knitr)\r\nlibrary(gganimate)\r\nlibrary(lubridate)\r\nlibrary(ggrepel)\r\nlibrary(widgetframe)\r\nlibrary(magick)\r\n\r\n\r\ndata <- read_csv(\"data/LSMU_publication_history.csv\")\r\n\r\ndata[data == \"n/a\"] <- NA\r\ndata <- data %>% mutate_at(vars(`Publication Date`:`Journal Impact Factor`), funs(as.numeric))\r\n\r\n\r\n\r\nThe dataset uses “n/a” instead of R’s NA, therefore some tidying is required. Because n/a is recognised as a character, we also need to turn some of the character columns to numeric. I will be doing some data wrangling later but for now this suits our needs.\r\nThe dataset does have a few limitations: InCites only lists the first five authors of each paper, limiting the analysis on authorship. Also, research area categories are provided by the InCites database and, unfourtunately, I do not have information about how they are assigned.\r\nPublishing throughout the years\r\nLithuania has gained independence in 1990 and LSMU has merged with the Veterinary academy in 2010 whatever. What is the distribution of papers throughout the years?\r\n\r\n\r\n\r\nInterestingly, in 1990 there were no major shifts and up until 1999 the rate of publication had been stable. I couldn’t find a clear explanation as to why the rate started increasing - my best guess is that around 1999, international exchange (such as Erasmus) programs had just started. As researchers, doctors and students travelled abroad to work or study, they either gained experience that enabled to conduct research or they established relationships with Western Europe that would facilitate publication.\r\nThe period of 2008-2011 is interesting - the number of publications increased despite the financial crisis. In 2010, LSMU (formerly, KMU) merged with the Lithuanian Veterinary Academy, could the merger be masking a decrease in the number of publications? Luckily, the dataset contains information about the research area of the publication. And as for 2018, the year seems to be slow compared to the previous year.\r\nHere is a plot with veterinary sciences seperated from the rest of the papers:\r\n\r\n\r\n\r\nI needed to use grepl because each paper is assigned multiple research areas. The dip we saw previously does seem larger! Let’s see how this tendency is reflected in different research areas. To do that, I will use dplyr’s separate and gather commands – absolutely indispensable when working with character vectors.\r\nWhich research areas are thriving?\r\nThere are a lot of research area categories - certainly not enough to plot them elegantly! Let’s try limiting it to the categories that have 150 or more publications.\r\n\r\n\r\n\r\nIt does seem cardiac sciences have a similar distribution compared to the overall publication distribution. After tinkering around with plots and failing to find the root cause of the dip, the question is, whose research output decreased the most?\r\n\r\n\r\n\r\nYeah, we can see cardiovascular sciences and engineering took the biggest hit, however, some of the research areas have produced more papers! I could probably investigate this further by building a linear model and then checking residuals, but this is fine as it is.\r\nQuality over quantity?\r\nIt is pretty evident that the volume of research has increased throughout the years and will probably keep increasing to a certain point. Of course, quality does not equal quantity and in the age of “publish or perish”, it would be interesting to check whether researchers publish more papers at the cost of quality.\r\nFirst, it’s interesting to note that almost half (45.32%) of all research papers do not get cited at all:\r\n\r\n\r\n\r\nIt would be interesting to check how this distribution compares to other universities, but I think that is a question left for another day. Of course, citations cannot solely be used to make decisions about the quality of research. One of the drawbacks is that new papers are very likely to be uncited for a couple of years. Furthermore, really old papers have the misfortune of not appearing in PubMed searches.\r\nInstead of using the number of citations as a metric, I will use InCites’ “Percentile in Subject Area”, which shows the relative position of your paper compared to those in the same field. Also, I am going to take the year of publication into account so that we can see how publishing changed over time. To combine all of this, let’s use gganimate! I am going to label the largest research areas in the graph - labelling all of them would clutter the plot!\r\n\r\n\r\n\r\nI inversed the percentiles in the plot - the 100th percentile is what every paper should be aiming for and the 0th percentile indicates that a paper has not been cited yet. It’s interesting to see small categories fade in and out of the plot. The plot also shows which areas suffer from publication bloat. In spite of that, around 2017, the quality of cardiovascular and public health papers increased slightly! Also, one of the bigger categories to uphold a higher percentile value is gastroenterology – kudos to them! Overall, the plot looks promising - even though the university produces research that gets outranked by other universities, over time, a lot of the categories have had their percentiles increase.\r\nWho works with whom?\r\nSince the publications have authors listed alongside in the data, I would like to know whether research groups work independently or cooperate with other groups. Unfourtunately, InCites provides only the first 5 authors of each paper. Obviously, this means that we lose any information about authors who are credited after the fifth author. But more importantly, usually papers list the supervisor as the last author. This means that some of the most important connections between authors may become lost. This may cause some of the author clusters to be isolated when in fact they are connected by someone. Finally, I have added a threshold of 4 publications to show common authorship. Adding more produced a highly interconnected plot that was hard to parse. With these caveats in mind, let’s look at clusters of authors!\r\n\r\n\r\n\r\n\r\npreserve5de2d3174cd74fc8\r\n\r\nLuckily, the clusters are not completely isolated, suggesting that research teams do collaborate at the university. It’s interesting to see which research groups work together – an obvious example that is reflected in the plot is pulmonology, allergology and laryngology. In certain clusters, it is pretty easy to see department heads because they are in the center of their respective clusters. Also, some teams do seem to work independently, however I wouldn’t make any conclusions based on the limitations of the dataset and the plot. Nonetheless, we did get some insights from the visualisation!\r\nThe plot was built using highcharter - I have tried to initially use plotly to make it interactive, however, coding in highcharter proved to be more convenient. Also, since each paper has multiple research areas, so too does each author have multiple areas. I have kept the research area in which an author has produced the most publications.\r\nIn conclusion\r\nWhew, we are done! What have we learned today?\r\nThe financial crisis of 2007–2008 did affect the research output.\r\nA large portion or research papers are left uncited!\r\nThe quality of papers in some of the research areas is lower at the cost of quantity.\r\nAs for the code, it can be found in the blog’s repository. What’s great is that this code will work with any dataset exported from InCites’ database. I might try and compare different universities (locally or globally) in the future!\r\n\r\n\r\n\r\n",
    "preview": "posts/2018-09-10-analysing-lsmu-publications/index_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-06-07T21:41:11+03:00",
    "input_file": "index.utf8.md"
  }
]
